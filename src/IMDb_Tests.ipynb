{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying our PreDeCon implementation to the IMDB-BINARY dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import csv\n",
    "import sys\n",
    "sys.path.append('../tudataset/tud_benchmark/')\n",
    "\n",
    "from predecon import PreDeCon\n",
    "from scipy.sparse import linalg\n",
    "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
    "from pathlib import Path\n",
    "from auxiliarymethods.auxiliary_methods import normalize_feature_vector as normalize_v\n",
    "from auxiliarymethods.auxiliary_methods import normalize_gram_matrix as normalize_m\n",
    "from auxiliarymethods.datasets import get_dataset as labels\n",
    "from sources.utility_functions import load_sparse as load_v\n",
    "from sources.utility_functions import load_csv as load_m\n",
    "from sources.dimensionality_reduction import truncatedSVD as svd\n",
    "from sources.dimensionality_reduction import kernelPCA as pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use NMI with regard to the ground truth labels to measure the performance of our algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = '../tudataset/datasets/IMDB-BINARY/IMDB-BINARY/raw/IMDB-BINARY_graph_labels.txt'\n",
    "true_labels = np.loadtxt(labels_path, dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis showed a significant number of outliers that were fully connected graphs. In trying to use this knowledge to our advantage, we have implemented a method to strip graphs with a high density from the dataset. (This is not used in our best clustering to date.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "densities = np.fromfile('densities.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not yet know for which kernel method and representation (vectors or gram matrix) the algorithms works best; the same goes for the reduced number of dimensions that we get after applying SVD or KernelPCA. Accordingly, we are treating these inputs as additional parameters to the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predecon_config(kernel, format, dims, minPts, eps, delta, lambda_, kappa, strip_densities=1):\n",
    "    imdb = Path('../graph_representations/without_labels/')\n",
    "    vector_path = imdb / f'IMDB-BINARY_vectors_{kernel}.npz'\n",
    "    matrix_path = imdb / f'IMDB-BINARY_gram_matrix_{kernel}.csv'\n",
    "\n",
    "    if format == 'vector':\n",
    "        vectors = normalize_v(load_v(vector_path))\n",
    "        data = svd(vectors, dims)\n",
    "    else:\n",
    "        matrix = normalize_m(load_m(matrix_path))\n",
    "        data = pca(matrix, dims)\n",
    "    \n",
    "    if strip_densities < 1:\n",
    "        # option to remove highly connected graphs from the dataset\n",
    "        data = data[densities < strip_densities]\n",
    "    \n",
    "    predecon = PreDeCon(minPts=minPts, eps=eps, delta=delta, lambda_=lambda_, kappa=kappa)\n",
    "    predecon.fit(data)\n",
    "    return predecon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is not quite straightforward to find parameters for which the algorithm returns usable clusterings. In many cases, the entire dataset is determined to be one big cluster; in other cases, each and every point is marked as noise.\n",
    "\n",
    "We have implemented a random parameter space search to get usable results (shown below). One of the first successful configurations that at least came up with the correct number of clusters (2 classes + noise) is shown here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters found: {1, 2, -1}\n",
      "NMI: 0.020830857906415683\n"
     ]
    }
   ],
   "source": [
    "kernel = 'wl3'\n",
    "format = 'matrix'\n",
    "dims = 50\n",
    "\n",
    "predecon = predecon_config(kernel, format, dims, 25, 0.75, 1, 50, 10)\n",
    "print(\"Clusters found:\", set(predecon.labels))\n",
    "print(f\"NMI: {NMI(true_labels, predecon.labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NMI of 0.02 shows hardly better-than-random performance.\n",
    "\n",
    "After some iterations of using the results to limit our parameter search space to more relevant ranges, we found this configuration, the best one yet. The NMI of 0.13 is still not exactly great, but it shows improvement.\n",
    "\n",
    "One interesting aspect of this configuration is that it returns 12 clusters, much more than in the ground truth. This shows room for improvement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clusters found: {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, -1}\n",
      "NMI: 0.1316886176993423\n"
     ]
    }
   ],
   "source": [
    "kernel = 'wl1'\n",
    "format = 'vector'\n",
    "dims = 25\n",
    "\n",
    "predecon = predecon_config(kernel, format, dims, 7, 5, 20, 25, 100)\n",
    "print(\"Clusters found:\", set(predecon.labels))\n",
    "print(f\"NMI: {NMI(true_labels, predecon.labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random parameter search is implemented here. The search ranges started out kind of randomly but were refined when we found some configurations that worked significantly better than average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_kernels = ['wl1', 'wl2', 'wl3', 'wl4', 'wl5', 'graphlet', 'shortestpath']\n",
    "all_formats = ['vector', 'matrix']\n",
    "all_dims    = [25, 50, 75]\n",
    "\n",
    "all_minPts  = [2, 5, 7, 10, 15, 25]\n",
    "all_eps     = [0.25, 0.75, 2, 5, 50]\n",
    "all_deltas  = [0.1, 0.25, 0.5, 1, 5, 20]\n",
    "all_lambdas = [5, 15, 30, 50, 75]\n",
    "all_kappas  = [10, 100, 1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we just choose one random element from each of the parameter lists and use this as the configuration for one trial run. If the configuration returns a usable clustering, the parameters and the resulting NMI are saved to a CSV file. We can use this file to find patterns in the successful clusterings to improve our search range for future runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0: No clusterings found\n",
      "Trial 1: \n",
      "  wl4 matrix 25\n",
      "  7 2 0.25 75 1000\n",
      "  {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, -1}\n",
      "  NMI: 0.08072790567747767\n",
      "  time: 0.3848s\n",
      "Trial 2: \n",
      "  wl1 matrix 75\n",
      "  25 2 0.1 75 1000\n",
      "  {1, 2, -1}\n",
      "  NMI: 0.020830857906415683\n",
      "  time: 0.6774s\n",
      "Trial 3: No clusterings found\n",
      "Trial 4: No clusterings found\n",
      "Trial 5: \n",
      "  wl2 matrix 25\n",
      "  2 2 0.1 30 100\n",
      "  {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, -1}\n",
      "  NMI: 0.10171255613695186\n",
      "  time: 0.4214s\n",
      "Trial 6: No clusterings found\n",
      "  Took too long…\n",
      "   shortestpath matrix 75\n",
      "   5 2 5 75 10\n",
      "  time: 161.4814s\n",
      "Trial 7: \n",
      "  wl2 vector 25\n",
      "  10 2 20 30 10\n",
      "  {1, 2, 3}\n",
      "  NMI: 0.021425527802015513\n",
      "  time: 3.1558s\n",
      "Trial 8: \n",
      "  wl5 vector 25\n",
      "  2 5 20 30 1000\n",
      "  {1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, -1}\n",
      "  NMI: 0.10597753084402024\n",
      "  time: 0.4627s\n",
      "Trial 9: No clusterings found\n",
      "  Took too long…\n",
      "   shortestpath matrix 25\n",
      "   5 50 5 50 1000\n",
      "  time: 256.6318s\n"
     ]
    }
   ],
   "source": [
    "# randomized parameter space search\n",
    "\n",
    "num_trials = 10\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    print(f\"Trial {trial}: \", end='')\n",
    "\n",
    "    kernel  = random.choice(all_kernels)\n",
    "    format  = random.choice(all_formats)\n",
    "    dims    = random.choice(all_dims)\n",
    "\n",
    "    minPts  = random.choice(all_minPts)\n",
    "    eps     = random.choice(all_eps)\n",
    "    delta   = random.choice(all_deltas)\n",
    "    lambda_ = random.choice(all_lambdas)\n",
    "    kappa  = random.choice(all_kappas)\n",
    "\n",
    "    predecon = predecon_config(kernel=kernel, format=format, dims=dims, \\\n",
    "            minPts=minPts, eps=eps, delta=delta, lambda_=lambda_, kappa=kappa)\n",
    "    \n",
    "    # true_labels_stripped = true_labels[densities < 0.99]\n",
    "    \n",
    "    if len(set(predecon.labels)) > 1:\n",
    "        nmi = NMI(true_labels, predecon.labels)\n",
    "\n",
    "        print(\"\\n \", kernel, format, dims)\n",
    "        print(\" \", minPts, eps, delta, lambda_, kappa)\n",
    "        print(\" \", set(predecon.labels))\n",
    "        print(\"  NMI:\", nmi)\n",
    "        print(f\"  time: {predecon._performance['fit'] / 1000_000_000:.4f}s\")\n",
    "\n",
    "        with open('parameters.csv', 'a') as f:\n",
    "            csv.writer(f).writerow([nmi, kernel, format, dims, minPts, eps, delta, lambda_, kappa])\n",
    "    else:\n",
    "        print(\"No clusterings found\")\n",
    "    \n",
    "    if predecon._performance['fit'] > 60 * 1000_000_000:\n",
    "        print(\"  Took too long…\")\n",
    "        print(\"  \", kernel, format, dims)\n",
    "        print(\"  \", minPts, eps, delta, lambda_, kappa)\n",
    "        print(f\"  time: {predecon._performance['fit'] / 1000_000_000:.4f}s\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "uni.dm.ga",
   "language": "python",
   "name": "uni.dm.ga"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
